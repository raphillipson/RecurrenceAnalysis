#!/usr/bin/python
# -*- coding: utf-8 -*-
#
# This file is part of recurrence_analysis.
# Insert copyright info.

"""
Provides classes for the analysis of time series data using methods from nonlinear dynamics
and recurrence plot quantification.
"""
import numpy as np
from scipy.spatial import distance
from scipy.spatial import cKDTree as KDTree
from scipy.spatial.distance import pdist, squareform
from scipy.interpolate import interp1d
import scipy.sparse
from scipy import fftpack
from astropy.convolution import convolve, Box1DKernel, Gaussian1DKernel
from itertools import chain


class Recurrence_Plot:

    """
    Class Recurrence_Plot for generating recurrence plots and computing their quantitative statistics.

    The RecurrencePlot class supports the construction of recurrence plots
    from multi-dimensional time series, optionally using embedding. Currently,
    manhattan, euclidean and supremum norms are provided for measuring
    distances in phase space.

    Methods for calculating commonly used measures of recurrence quantification analysis (RQA) are included:
    determinism, laminarity, maximum and average diagonal(vertical) line lengths, Shannon entropy, divergence,
    RATIO, recurrence rate, and tau recurrence rate (close returns).
    """

    #
    #  Internal methods
    #
    
    def __init__(self, time_series, multidim=False, metric="euclidean", normed=True, 
                 **kwds):

        """
        Initialize an instance of Recurrence_Plot.

        A dimension and delay are required by which the time series ('time_series') is embedded in phase space 
        using the Time Delay method.
        If not specified, the default dimension and delay values are each 1 and the recurrence plot 
        will reduce to a standard correlation function.

        If a multi-dimensional time series is provided (or one already embedded), then multidim must be flagged to True.

        By default, the recurrence plot is generated by a given recurrence rate (provided by 'threshold'). A recurrence plot can be generated using
        a specific threshold value corresponding to an absolute distance between positions in phase space by specifying threshold_by="distance".

        By default, the time series is normalized by a Zscore (e.g. mean-subtracted and divided by the standard deviation).

        :type time_series:     2D array (time, dimension)
        :arg time_series:      The time series to be analyzed, must be scalar.
        :arg int dimension:    The embedding dimension to use with Time Delay method
        :arg int delay:        The embedding time delay to use with Time Delay method
        :arg int theiler:      The Theiler window
        :arg bool multidim:    Indicate multidimensional input (True); indicate scalar input (False) and use Time Delay method
        :arg number threshold: The threshold keyword for generating
                               the recurrence plot using a fixed recurrence rate (if by_threshold='recurrence_rate') 
                               or absolute distance (if by_threshold='distance')
        :arg str metric:       The metric for measuring distances in phase space
                               ("manhattan", "euclidean", "supremum").
        :arg str threshold_by: The method for determining entries in the recurrence matrix
                               ("recurrence_rate", "threshold").
        :arg bool normed:      Decide whether to normalize the time series to
                               zero mean and unit standard deviation.
        :arg str norm:         Default 'Zscore' (zero mean, unit standard deviation); 
                               alternative: 'minmax' (subtract min, divide by amplitude)
        """
        if type(time_series) is not np.ndarray:
            raise NameError('input time series must by a numpy array. exiting.') 

        #  Store time series
        self.time_series = time_series.copy().astype(np.float)

        #  Reshape time series
        self.time_series.shape = (self.time_series.shape[0], -1)

        #  Store type of metric used for measuring distances between points in embedded phase space
        self.metric = metric

        #  Normalize time series by the Zscore (zero mean and unit standard deviation)
        if normed:
            self.normalize(self.time_series)

        #  Get embedding dimension and delay (for use with Time Delay method) from **kwds
        self.dimension = kwds.get("dimension")
        self.delay = kwds.get("delay")

        #  Embed the time series if necessary, or use multidimensional input as embedded array
        if multidim:
            self.embedded_ts = self.time_series
        else:
            if self.dimension is None or self.delay is None:
                raise NameError("Provide both time delay and embedding dimension.")
            else:
                self.embedded_ts = self.embed(self.time_series, self.dimension, self.delay)

        # Establish the Theiler window
        self.theiler = kwds.get("theiler")
        if self.theiler is None:
            self.theiler = 0

        # The size of the recurrence matrix (it is square)
        self.N = self.embedded_ts.shape[0]

        # The recurrence matrix
        self.RP = None
        self._distance_matrix = None

        #  Get threshold or recurrence rate from **kwds
        self.threshold = kwds.get("threshold")
        self.threshold_by = kwds.get("threshold_by")


        #  Precompute recurrence matrix 
        if self.threshold is not None:
            #  Calculate the recurrence matrix RP using the radius of neighborhood threshold
            if self.threshold_by is not None:
                if self.threshold_by == "distance":
                    Recurrence_Plot.rp_threshold_by_distance(self, self.embedded_ts, self.threshold, self.metric)
                #  Calculate the recurrence matrix RP using a fixed recurrence rate
                elif self.threshold_by == "recurrence_rate":
                    Recurrence_Plot.rp_threshold_by_recurrence_rate(self, self.embedded_ts, self.threshold, self.metric)
            else:
                # if not specified, used fixed recurrence rate
                Recurrence_Plot.rp_threshold_by_recurrence_rate(self, self.embedded_ts, self.threshold, self.metric)
        else:
            raise NameError("Please give a threshold to construct the recurrence plot!")


            
    def __str__(self):
        """
        Returns a string representation.
        """
        return ('RecurrencePlot: time series shape %s.\n'
                'Embedding dimension %i\nThreshold %s, %s metric') % (
                    self.time_series.shape, self.dim if self.dim else 0,
                    self.threshold, self.metric)
   
    def recurrence_matrix(self):
        """
        Return the current recurrence matrix 
        """
        return self.RP


    @staticmethod
    def embed(x, dimension=1, delay=1):
        """
        Embed a scalar time series using the time delay method
        Inputs:
            x         : input time series (numpy array or list)
            dimension : embedding dimension (int)
            delay     : embedding time delay (int)
        Outputs:
            z         : embedded time series : 
                        [ (x(t1), x(t1+i), ... x(t1+m*i)), (x(t2), x(t2+i), ... x(t2+m*i)), ... ]
        """
        N = len(x)
        k = N - (dimension - 1) * delay
        z = np.zeros((k, dimension), dtype="float")
        for i in range(k):
            z[i] = [x[i + j * delay] for j in range(dimension)]

        return np.array(z)


    @staticmethod
    def normalize(x):
        '''
        Normalize a time series to have zero mean and unit standard deviation.
        Inputs:
            x : input time series (numpy array or list)
        Outputs:
            z : normalized time series (numpy array)
        '''
        if type(x) is list:
            x = np.array(x)
        if type(x) is not np.ndarray:
            print('input time series must by a numpy array or list. exiting.')
            return 

        # normalize time series 
        z = (x - x.mean()) / x.std()

        return z

    #################################
    # Calculate the Recurrence Plot
    #################################
    def distance_matrix(self, embedded_time_series, metric):
        """
        Return phase space distance matrix using a specified metric
        Inputs:
            embedded_time_series : (numpy array) The embedded phase space trajectory of the time series
            metric               : (str) The metric for measuring distances in phase space
                                       ("manhattan", "euclidean", "supremum", etc 
                                       from pdist in scipy.spatial.distance).
        Outputs
            :distance matrix: the phase space distance matrix :math:`D`
        """
        # pdist: Pairwise distances between observations in n-dimensional space.
        #        metric str or function, optional -- The distance metric to use. 
        #        The distance function can be:
        #            ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, 
        #            ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’, 
        #            ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, 
        #            ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.
        # square form: converts between condensed distance matrices and square distance matrices
        distances = squareform(pdist(embedded_time_series, metric=metric))
        self._distance_matrix = distances
        
        return distances
    
    def rp_threshold_by_distance(self, embedded_x, threshold, metric):
        """
        Returns the recurrence plot of given time series.
        Inputs:
            embedded_x   : embedded time series (numpy array or list)
            threshold    : distance threshold for recurrence plot (float)
            metric       : (str) distance metric for determining distance between pairs of observations in the embedded time series
                           options are from scipy.spatial.distance.pdist (see Recurrence_Plot.distance_matrix)    
        Outputs:
            2D recurrence matrix (2D numpy array)
        """
        distances = Recurrence_Plot.distance_matrix(self, embedded_x, metric)

        # create empty matrix of same size as the distance matrix
        recurrence_matrix = np.zeros(distances.shape, dtype="int")

        # strict cut-off at the threshold distance to determine non-zero RP values
        i = np.where(distances <= threshold)  
        recurrence_matrix[i] = 1

        self.RP = recurrence_matrix
    

    def rp_threshold_by_recurrence_rate(self, embedded_x, threshold, metric):
        """
        Returns the recurrence plot of given time series.
        
        Inputs:
            embedded_x   : embedded time series (numpy array or list)
            threshold    : recurrence rate threshold for recurrence plot (float between 0.0 and 1.0)
            metric       : (str) distance metric for determining distance between pairs of observations in the embedded time series
                           options are from scipy.spatial.distance.pdist (see Recurrence_Plot.distance_matrix)    
        Outputs:
            2D recurrence matrix
        """
        distances = Recurrence_Plot.distance_matrix(self, embedded_x, metric)

        # create empty matrix of same size as the distance matrix
        recurrence_matrix = np.zeros(distances.shape, dtype="int")

        # convert threshold to a percentile and use that as a cut-off
        #  to determine non-zero RP values
        # i.e.: np.percentile computes the q-th percentile (in this case threshold*100) of the distance matrix
        # if threshold = 0.2, then we compute the 20th percentile of the distances and use that as the cutoff for the RP
        if threshold <= 0.0 or threshold >= 1.0:
            raise NameError("Threshold using constant recurrence rate requires a value between 0.0 and 1.0. Exiting.")
        threshold = np.percentile(distances, threshold * 100.)
        i = np.where(distances <= threshold)
        recurrence_matrix[i] = 1

        self.RP = recurrence_matrix

        
    def threshold_from_recurrence_rate(self, recurrence_rate, rr_precision=0.001):
        """
        Method adapted from pyunicorn code.
        Return the threshold for recurrence plot construction given the recurrence rate
        by randomly selected rr_precision percent of the distance matrix entries.

        Inputs:
            recurrence_rate : The desired recurrence rate (float)
            rr_precision    : The desired precision of recurrence rate estimation (float)
        Outputs:
            threshold       : The recurrence threshold corresponding to the desired recurrence rate (float)    
        """
        distance_matrix = self._distance_matrix
        
        #  Get number of distances to be randomly chosen
        n_samples = int(rr_precision * distance_matrix.size)

        #  Get number of phase space points
        n_time = distance_matrix.shape[0]

        # vectorized version
        i = np.random.randint(n_time, size=n_samples)
        j = np.random.randint(n_time, size=n_samples)
        samples = distance_matrix[i, j]

        #  Sort and get threshold
        samples.sort()
        threshold = samples[int(recurrence_rate * n_samples)]
        return threshold

    
    #################################################
    # Calculate the Recurrence Plot statistics (RQA)
    #################################################
    def recurrence_rate(self):
        ''' 
        Recurrence Rate -- the percentage of recurrence points in the RP corresponds to the correlation sum.
        Returns the fraction of non-zero entries in the recurrence matrix (float)
        '''
        rp_matrix = self.recurrence_matrix()
        N = self.N
        RR = np.nansum(rp_matrix)/float(N*N)
        
        self.recurrence_rate = RR
        
        return RR
    
    def recurrence_probability(self, lag=0):
        """
        Recurrence probability -- the probability that the trajectory is recurrent after 'lag' time steps
        Inputs:
            lag (int) : an integer that is the offset from the main diagonal of the recurrence matrix
        Returns:
            for the diagonal that is offset by the main diagonal of the recurrence matrix by 'lag' steps, returns
            the recurrence rate (float)
        """
        RP = self.recurrence_matrix()
        N = self.N
        SUM = np.sum(np.diag(RP, lag))

        return SUM / float(N-lag)
    
    
    @staticmethod
    def max_consecutive_elements(vector):
        ''' 
        Helper function for line_distributions
        Find consecutive non-zero values in an array
        '''
        # extract the length of all consecutive elements in a vector
        tmp = np.diff(vector)
        ind1 = np.where(tmp == 1)[0]
        ind2 = np.where(tmp == -1)[0]
        consElements = ind2-ind1

        return consElements


    def line_distributions(self, theiler=1, minLenDiag=0, minLenVert=0):
        '''
        input:
            theiler : size of the theiler window (int, default: 1)
            minL    : minimum diagonal line length (int, default: 0)
            minV    : minimum vertical line length (int, default: 0)

        output:
            diagLines : distribution of diagonal lines (array)
            vertLines : distribution of vertical lines (array)
        '''
        
        RP_matrix = self.recurrence_matrix()
        N = self.N
        
        # remove theiler window
        if theiler > 0:
            RP = np.triu(RP_matrix,theiler) + np.tril(RP_matrix,-theiler)
        else:
            RP = RP_matrix

        # get diagonal lines and pad with zeros
        diags_L = np.array( [np.concatenate((np.diag(RP, k=i), np.zeros(np.abs(i)))) for i in range(-N+1, 1, 1)] )
        diags_U = np.array( [np.concatenate((np.zeros(np.abs(i)), np.diag(RP, k=i))) for i in range(1, N, 1)] )
        diagonals = np.concatenate((diags_L, diags_U))
        diagonals = np.concatenate( ([np.zeros(len(diagonals))], diagonals.T, [np.zeros(len(diagonals))]) )

        # get vertical lines and pad with zeros
        verticals = np.concatenate( (np.zeros((1,N)), RP, np.zeros((1,N))) )

        # get the actual line distributions
        diagLines = Recurrence_Plot.max_consecutive_elements(diagonals.T.flatten())
        vertLines = Recurrence_Plot.max_consecutive_elements(verticals.T.flatten())

        # exclude short lines
        if minLenDiag > 0:
            diagLines = diagLines[diagLines >= minLenDiag]
        if minLenVert > 0:
            vertLines = vertLines[vertLines >= minLenVert] 

        return diagLines, vertLines
    
    @staticmethod
    def line_histogram(line_distributions, line_type='diagonal'):
        """
        Helper function for creating histograms of line_distributions
        line_histogram -- the histogram P(l) of lines of length l dervied from the recurrence plot

        Inputs:
            line_distributions : the distributions of diagonal and vertical line distributions (see function lineDists)
            line_type          : 'diagonal' or 'vertical' -- determines whether we return a diagonal or vertical 
                                 line histogram
        Outputs:
            num_lines, bins, line_lengths : the parameters from a numpy.histogram object of diagonal(vertical) line lengths

        """
        diagLines, vertLines = line_distributions

        if line_type == 'diagonal':
            bins = np.arange(0.5, diagLines.max() + 0.1, 1.0)
            num_lines, _ = np.histogram(diagLines, bins=bins)
        elif line_type == 'vertical':
            bins = np.arange(0.5, diagLines.max() + 0.1, 1.0)
            num_lines, _ = np.histogram(diagLines, bins=bins)
        else:
            print("Line type not specified (must be line_type='diagonal' or 'vertical')")

        return num_lines, bins



    def DET(self, l_min=2, theiler=1):
        '''
        DET -- the Determinism (DET) quantity derived from the recurrence plot
               This is defined as the fraction of recurrence points that are part of diagonal lines versus isolated recurrence points.
               Definition:
                            Sum(l = lmin to N) l*P(l)
                    DET =  ---------------------------
                             Sum(l = 1 to N) l*P(l)
                 where l = diagonal line length, P(l) is the histogram of diagonal line lengths, 
                 lmin is a minimum line length (must be greater than 1), N is the total number of phase space points

        Inputs:
            theiler         : theiler window of recurrence matrix (int)
            l_min           : minimum length of a line to consider as a diagonal line in DET (int)
        Outputs:
            DET             : (float) the determinism

        '''        
        # compute the diagonal line lengths from the rp
        diagLines, _ = Recurrence_Plot.line_distributions( self, theiler=theiler )

        # total number of line lengths greater than l_min, and total number of line lengths
        sumDL = np.nansum(diagLines)
        diagReference = diagLines[diagLines >= l_min]

        # determinsim is the ratio
        determinism = np.nansum(diagReference)/sumDL    

        return determinism


    def LAM(self, theiler=1, v_min=2):
        '''
        LAM -- the Laminarity (LAM) quantity derived from the recurrence plot
               This is defined as the fraction of recurrence points that are part of vertical lines versus isolated recurrence points.
               Definition:
                            Sum(v = vmin to N) v*P(v)
                    LAM =  ---------------------------
                             Sum(v = 1 to N) v*P(v)
                 where v = vertical line length, P(v) is the histogram of vertical line lengths, 
                 vmin is a minimum line length (must be greater than 1), N is the total number of phase space points

        Inputs:
            theiler         : theiler window of recurrence matrix (int)
            v_min           : minimum length of a line to consider as a vertical line in LAM (int)
        Outputs:
            LAM             : (float) the laminarity

        '''        
        # compute the diagonal line lengths from the rp
        _, vertLines = Recurrence_Plot.line_distributions( self, theiler=theiler, minLenVert=1 )

        # total number of line lengths greater than l_min, and total number of line lengths
        sumVL = np.nansum(vertLines)
        vertReference = vertLines[vertLines >= v_min]

        # determinsim is the ratio
        laminarity = np.nansum(vertReference)/sumVL    

        return laminarity


    def Lavg(self, l_min=2, theiler=1):
        '''
        Lavg -- the average length of the diagonal lines (Lavg) quantity derived from the recurrence plot
                 Definition:
                            Sum(l = lmin to N) l*P(l)
                    Lavg =  ---------------------------
                             Sum(l = lmin to N) P(l)
                 where l = diagonal line length, P(l) is the histogram of diagonal line lengths, 
                 lmin is a minimum line length (must be greater than 1), N is the total number of phase space points

        Inputs:
            theiler         : theiler window of recurrence matrix (int)
            l_min           : minimum length of a line to consider as a diagonal line (int)
        Outputs:
            Lavg             : (float) the determinism

        '''        
        # compute the diagonal line lengths from the rp
        diagLines, _ = Recurrence_Plot.line_distributions( self, theiler=theiler, minLenDiag=l_min )

        # total number of line lengths greater than l_min, and total number of line lengths
        diagReference = diagLines[diagLines >= l_min]

        # Lmean is merely the average
        Lmean = np.nanmean(diagReference)   

        return Lmean


    def TT(self, theiler=1, v_min=2):
        '''
        TT -- the trapping time (TT) quantity derived from the recurrence plot
               This is defined as the average length of the vertical lines.
               Definition:
                            Sum(v = vmin to N) v*P(v)
                    TT  =  ---------------------------
                             Sum(v = vmin to N) P(v)
                 where v = vertical line length, P(v) is the histogram of vertical line lengths, 
                 vmin is a minimum line length (must be greater than 1), N is the total number of phase space points

        Inputs:
            theiler         : theiler window of recurrence matrix (int)
            v_min           : minimum length of a line to consider as a vertical line (int)
        Outputs:
            TT              : (float) the trapping time 

        '''
        # compute the diagonal line lengths from the rp
        _, vertLines = Recurrence_Plot.line_distributions( self, theiler=theiler, minLenVert=v_min )

        # total number of line lengths greater than l_min, and total number of line lengths
        vertReference = vertLines[vertLines >= v_min]

        # TT is merely the average
        Vmean = np.nanmean(vertReference)    

        return Vmean


    def shannon_entr(self, theiler=1, l_min=2):
        '''
        shannon_entr --- the Shannon entropy of the probability distribution of the diagonal line lengths P(l)
                 Definition:
                      ENTR =  - Sum(l = lmin to N) P(l) log(P(l))

                 where l = diagonal line length, P(l) is the histogram of diagonal line lengths, 
                 lmin is a minimum line length (typically greater than 1), N is the total number of phase space points,
                 and log is the natural logarithm
        Inputs:
            theiler         : theiler window of recurrence matrix (int)
            l_min           : minimum length of a line to consider as a diagonal line (int)
        Outputs:
            shannon_entr    : (float) the shannon entropy 
        '''        
        # each entry in diagLines is a line length 
        diagLines, _ = Recurrence_Plot.line_distributions( self, theiler=theiler )

        # the bins should be the EDGES of the line lengths ranging from 1 to the longest line length
        # (should be 0.5, 1.5, 2.5,...)
        bins = np.arange(0.5, diagLines.max() + 0.1, 1.0)

        # num_lines gives us how many lines are of each length
        num_lines, _ = np.histogram(diagLines, bins=bins)

        # probability distribution of the diagonal line lengths
        line_density = num_lines.astype('float') / float(len(diagLines))

        # now we need the centers of each bin (should be 1, 2, 3, ...)
        line_lengths = (0.5 * (bins[:-1] + bins[1:])).astype('int')

        # choose lines longer than chosen minimum
        line_density = line_density[line_lengths >= l_min]

        # compute entropy: sum of P(l) * log( P(l) )   --- where P(l) = line density
        # and enforce positive values
        ENTR = (-line_density[line_density > 0.0] * np.log(line_density[line_density > 0.0])).sum()

        return ENTR

    def rqa_stats( self, l_min=2, v_min=2, theiler=1, silence=False ):
        '''
        rqa_stats ---- compute and return all the recurrence quantitative analysis statistics from the recurrence plot
        See DET, TT, LAM, Lavg, Shannon_entr for definitions.
        Also returns:
            Lmax  : the longest diagonal line length 
            Vmax  : the longest vertical line length
            DIV   : the divergence (1.0 / Lmax)
            RATIO : the ratio of the determinism to the recurrence rate
        '''
        rp_matrix = self.recurrence_matrix()
        N = self.N
        
        # get the vertical & diagonal line distributions (all length)
        diagLines, vertLines = Recurrence_Plot.line_distributions(self, theiler)

        sumDL = np.nansum(diagLines)
        sumVL = np.nansum(vertLines)

        # ensure min line lengths
        diagReference = diagLines[diagLines >= l_min]
        vertReference = vertLines[vertLines >= v_min]

        # Recurrence Rate -- the percentage of recurrence points in the RP
        # corresponds to the correlation sum
        N = len(rp_matrix)
        RR = np.nansum(rp_matrix)/(N*N)

        # TO DO: TREND -- the paling of the RP towards its edges

        # Diagonal line based quantities
        DET = np.nansum(diagReference)/sumDL
        Lmean = np.nanmean(diagReference)
        Lmax = np.nanmax(diagReference)
        DIV = 1.0/Lmax
        RATIO = DET/RR

        # Vertical line based quantities
        LAM = np.nansum(vertReference)/sumVL
        TT = np.nanmean(vertReference)
        Vmax = np.nanmax(vertReference)

        # Shannon Entropy
        bins = np.arange(0.5, diagLines.max() + 0.1, 1.0)
        num_lines, _ = np.histogram(diagLines, bins=bins)
        line_density = num_lines.astype('float') / float(len(diagLines))
        line_lengths = (0.5 * (bins[:-1] + bins[1:])).astype('int')
        line_density = line_density[line_lengths >= l_min]
        ENTR = (-line_density[line_density > 0.0] * np.log(line_density[line_density > 0.0])).sum()

        if not silence:
            print('''
        -----------------------------------------------------------------
        Recurrence Quantitative Analysis statistics:

            Minimum diagonal line length: %d
            Minimum vertical line length: %d
            Theiler window size: %d

            Recurrence Rate: %.3f
            Determinism: %.3f
            Laminarity: %.3f
            Shannon Entropy: %.3f
            Ratio (DET/RR): %.3f

            Average Diagonal Line Length: %.1f
            Trapping Time: %.1f
            Maximum Diagonal Line Length: %.1f
            Maximum Vertical Line Length: %.1f
            Divergence (1/L): %.3f
        -------------------------------------------------------------------
            ''' %(l_min, v_min, theiler, RR, DET, LAM, ENTR, RATIO, Lmean, TT, Lmax, Vmax, DIV))


        return RR, DET, LAM, ENTR, RATIO, Lmean, TT, Lmax, Vmax, DIV

    #########################################################
    # Functions for confidence intervals of RQA statistics
    #########################################################
    
    '''
    These functions are adapted from a Matlab program "rqaci" which determines the confidence intervals for
    the recurrence plot quantities that are based on diagonal/vertical line distributions. The confidence intervals are determined 
    using a bootstrap resampling technique.

    Reference: 
     S. Schinkel, N. Marwan, O. Dimigen & J. Kurths (2009): 
     "Confidence Bounds of recurrence-based complexity measures"
     Physics Letters A,  373(26), pp. 2245-2250
    '''

    @staticmethod
    def bootstrap(X, nBoot, seed=None):
        '''
        Helper function for rqa_ci
        bootstrap -- Resample data with replacement
        input:
            X        : numpy vector/matrix
            nSamples : number of resamplings (int)
            seed     : random number generator seed (int)

        output:
            y        : numpy matrix holding the resampled vectors    
        '''

        if type(X) is list:
            X = np.array(X)
        if type(X) is not np.ndarray:
            print('input must be a numpy array or list. exiting.')
            return 

        if seed is not None:
            rng = np.random.default_rng(seed=seed)
        else:
            rng = np.random.default_rng()

        if nBoot is None:
            nBoot = 10

        sizeIn = np.shape(X)

        if len(sizeIn) > 2:
            print('exit(1) -- input data can be a vector or a 2D matrix only')
            return 1

        if len(sizeIn) == 1:
            flagIsVector = 1
        else:
            flagIsVector = 0

        if flagIsVector:
            output = X[ np.ceil(  (max(sizeIn)-1)*rng.random((max(sizeIn), nBoot))  ).astype(int) ] 
        else:
            output = X[ np.hstack(np.ceil( sizeIn(1)*sizeIn(2)*rng.random((sizeIn[0],sizeIn[1],nBoot)) ).astype(int)) ]

        return output

    @staticmethod
    def percentile(X, Nk):
        '''
        Helper function fo rqa_ci
        percentile -- given a bootstrap sample & desired % confidence, gives confidence bounds
        Inputs:
            X  : numpy vector/matrix of bootstrap sample (e.g. array of DET values)
            Nk : numpy array/scalar of confidence interval (percentile range, e.g. [99.5,0.5], for 1% confidence range) 
        Outputs:
            p  : array of percentile values (upper, lower)
        '''
        X = np.squeeze(X)

        if len(Nk)!=len(Nk.flatten()):   # wrong Nk                
            print('exit(1) -- Nk must either be a scalar or a vector')
            return 1
        elif (len(X)==len(X.flatten())) and (np.shape(X)[0]==1) :     # X is 1D row
            X = X.T

        x = np.concatenate(([0], (np.linspace(0.5, (len(X)-0.5), num=len(X))/len(X)) * 100, [100]))
        y = np.concatenate(( [min(X)], np.sort(X), [max(X)] ))
        p = interp1d(x, y)(Nk)

        return p


    def rqa_ci( self, nBoot=500, alpha=1, theiler=1, minLenDiag=2, minLenVert=2 ):
        '''
        rqa_ci -- computes confidence intervals for line-distribution based RP quantities 
        
        input: 
            nBoot   : number of bootstrap samples (int, default: 500)
            alpha   : confidence level in % (number, default: 5-two-sided)
            theiler : size of the theiler window (int, default: 1)
            minL    : minimum length diagonal lines in RQA (int, default: 2)
            minV    : minimum length vertical lines in RQA (int, default: 2)

        output:
            refs(1) : referenced values for DET ( fraction of diagonal lines in RP )
            refs(2) : reference values for L (average length of diagonal line)
            refs(3) : reference values for LAM ( fraction of vertical lines in RP )
            refs(4) : reference values for TT (average length of vertical line)
            ci      : confidence bounds of values in refs
        '''
        #RP_matrix = self.recurrence_matrix()
        
        # confidence bounds
        confBounds = np.array([100-alpha/2, 0+alpha/2.0])

        # get the line distributions (all length)
        [diagLines, vertLines] = Recurrence_Plot.line_distributions(self, theiler)

        # compute RQA reference values
        sumDL = np.nansum(diagLines)
        sumVL = np.nansum(vertLines)

        diagReference = diagLines[diagLines >= minLenDiag]
        vertReference = vertLines[vertLines >= minLenVert]

        ref_DET = np.nansum(diagReference)/sumDL
        ref_Lmean = np.nanmean(diagReference)
        ref_LAM = np.nansum(vertReference)/sumVL
        ref_TT = np.nanmean(vertReference)

        refs = np.array([ ref_DET, ref_Lmean, ref_LAM, ref_TT ])

        # Boot strap one sample at a time, otherwise it kills the box

        # allocate the boostrapped values
        bsDET = np.zeros(nBoot)
        bsL = np.zeros(nBoot)
        bsLAM = np.zeros(nBoot)
        bsTT = np.zeros(nBoot)

        for i in range(0,nBoot,1):

            # bootstrap line distribution
            bsDistDl = Recurrence_Plot.bootstrap(diagLines,1)
            bsDistVl = Recurrence_Plot.bootstrap(vertLines,1)

            # total number of lines in bootstrapped distributions
            sumTempDL = np.nansum(bsDistDl)
            sumTempVl = np.nansum(bsDistVl)

            # exclude too short lines
            bsDistDl = bsDistDl[bsDistDl >= minLenDiag] 
            bsDistVl = bsDistVl[bsDistVl >= minLenVert] 

    #         bsDistDl[bsDistDl < minLenDiag] = []
    #         bsDistVl[bsDistVl < minLenVert] = []

            # complexity measures
            bsDET[i] = np.nansum(bsDistDl)/sumTempDL
            bsL[i] = np.nanmean(bsDistDl)
            bsLAM[i] = np.nansum(bsDistVl)/sumTempVl
            bsTT[i] = np.nanmean(bsDistVl)

        ci_DET = Recurrence_Plot.percentile(bsDET, confBounds)
        ci_Lmean = Recurrence_Plot.percentile(bsL, confBounds)
        ci_LAM = Recurrence_Plot.percentile(bsLAM, confBounds)
        ci_TT = Recurrence_Plot.percentile(bsTT, confBounds)

        ci = np.array( [ci_DET, ci_Lmean, ci_LAM, ci_TT] )

        return refs, ci
    
    
#########################################################
# Functions for time series pre-processing and analysis
#########################################################    

class Time_Series:

    """
    Class Time_Series for generating embedded time series and associated statistics.

    The Time_Series class supports the embedding of time series, re-binning, sub-sampling, 
    autocorrelation function, power spectra, numerical derivatives, plots of embedded or differential phase space,
    Legendre polynomial construction of embedded time series, dynamic power spectra, low/high frequency filtering,
    generation of surrogate data, generalized mutual information for determining nonlinear correlations and optimal embedding delay, 
    false nearest neighbors algorithm and average false neighbors for determining optimal embedding dimension
    
    References:
        For general nonlinear time series analysis approach (e.g. surrogate data method, time delay embedding):
            TISEAN: https://www.pks.mpg.de/tisean//Tisean_3.0.1/index.html
            T. Schreiber and A. Schmitz, Surrogate time series, Physica D 142 346 (2000) 
            R. Hegger, H. Kantz, and T. Schreiber, Practical implementation of nonlinear time series methods: The TISEAN package, CHAOS 9, 413 (1999) 
        Modified functions for computation of optimal embedding parameters (ADFD, FNN):
            NOLITSA: https://github.com/manu-mannattil/nolitsa
            M. Mannattil, H. Gupta, and S. Chakraborty, “Revisiting Evidence of Chaos in X-ray Light Curves: The Case of GRS 1915+105,” Astrophys. J. 833, 208 (2016).
    """

    #
    #  Internal methods
    #
    
    def __init__(self, time_series, times, errors=None, binTimes=None, multidim=False, normed=True, silence=False,
                 **kwds):

        """
        Initialize an instance of Time_Series.

        A dimension and delay are required by which the time series ('time_series') is embedded in phase space 
        using the Time Delay method.
        If not specified, the default dimension and delay values are each 1 

        If a multi-dimensional time series is provided (or one already embedded), then multidim must be flagged to True.

        By default, the time series is normalized by a Zscore (e.g. mean-subtracted and divided by the standard deviation).

        :type time_series:     2D array (time series y values (e.g. amplitude), dimension)
        :type times:           1D array (time series x values (e.g. time))
        
        Inputs:
            :arg time_series:      The time series to be analyzed, scalar or multi-dimensional
            :arg times:            The times for the input time_series, must be scalar
            :arg errors:           The errors for the input time_series
            :arg binTimes:         (optional) input array of exposure times for each observation centered on times
            :arg bool multidim:    Indicate multidimensional input (True); indicate scalar input (False) and use Time Delay method
            :arg bool normed:      Decide whether to normalize the time series to
                                   zero mean and unit standard deviation.
        Keywords:
            
            :arg int dimension:    The embedding dimension to use with Time Delay method
            :arg int delay:        The embedding time delay to use with Time Delay method

            :arg str norm:         Default 'Zscore' (zero mean, unit standard deviation); 
                                   alternative: 'minmax' (subtract min, divide by amplitude)
            :arg bool silence:     suppress print statements and output
        """
        if type(time_series) is not np.ndarray:
            raise NameError('input time series must by a numpy array. exiting.') 

        #  Store time series
        self.time_series = time_series.copy().astype(np.float)

        #  Get the errors and store
        if errors is not None:
            if normed:
                errors = errors / np.std(time_series)
            self.errors = errors.copy().astype(np.float)
        
        # Time series times
        self.times = times.copy().astype(np.float)

        #  Normalize time series by the Zscore (zero mean and unit standard deviation)
        if normed:
            self.normalize(self.time_series)

        #  Get embedding dimension and delay (for use with Time Delay method) from **kwds
        self.dimension = kwds.get("dimension")
        self.delay = kwds.get("delay")
        
        # Exposure times for each observation in the time series
        self.bin_times = binTimes
        
        #  Embed the time series if necessary, or use multidimensional input as embedded array
        if multidim:
            self.embedded_ts = self.time_series
        else:
            if self.dimension is None or self.delay is None:
                if not silence:
                    print("Embedding dimension or delay is not set. Defaulting to delay=1, dimension=1.")
                self.embedded_ts = self.embed(self.time_series, 1, 1)

        # The size of the recurrence matrix (it is square)
        self.N = self.embedded_ts.shape[0]


    def time_series(self):
        """
        Return the current time series
        """
        return self.time_series

    def times(self):
        """
        Return the current time series times
        """
        return self.times
    
    
    @staticmethod
    def normalize(x, err=None):
        '''
        Normalize a time series to have zero mean and unit standard deviation.
        Inputs:
            x : input time series (numpy array or list)
            err : input errors on time series (numpy array or list)
        Outputs:
            z : normalized time series or errors
        '''
        if type(x) is list:
            x = np.array(x)
        if type(x) is not np.ndarray:
            print('input time series must by a numpy array or list. exiting.')
            return 

        # normalize time series (or errors)
        if err is not None:
            zerr = err / x.std()
            return zerr
        else:
            z = (x - x.mean()) / x.std()
            return z
    
    @staticmethod
    def embed(x, dimension=1, delay=1):
        """
        Embed a scalar time series using the time delay method
        Inputs:
            x         : input time series (numpy array or list)
            dimension : embedding dimension
            delay     : embedding time delay 
        Outputs:
            z         : embedded time series 
                        [ (x(t1), x(t1+i), ... x(t1+m*i)), (x(t2), x(t2+i), ... x(t2+m*i)), ... ]
        """
        m = len(x) - (dimension - 1) * delay
        if m <= 0:
            raise ValueError('Length of the time series is <= (dim - 1) * tau.')

        return np.asarray([x[i:i + (dimension - 1) * delay + 1:delay] for i in range(m)])

    #
    #  Time series pre-processing methods
    #
    def findOutliers(self, name=None, work_dir='.', numStdDevs=3.0):
        """ findOutliers -- This determines the entries in a list/array of data that are
            outside 3-sigma of the data set, returns the indices of these entries and
            also writes them to a file.
            Input: 
                name       : (str) name of the data (e.g. Her X-1)
                work_dir   : (str) working directory to save outlier information
                numStdDevs : (float) number of standard deviations to use as threshold for outlier designation (default: 3.0)
            Output: returns a list of data values
                outliers   : (list) outliers indices
        """
        inputData = self.time_series
        inputErr = self.errors
        inputTimes = self.times
        inputBinTimes = self.bin_times
        
        # Get mean and standard deviation of the data
        mean = np.nanmean(inputData)
        stdDev = np.nanstd(inputData)
        
        if not silence:
            print('mean flux:', mean)
            print('stdDev flux:', stdDev)
        
        if inputErr is not None:
            stdDevErr = np.nanstd(inputErr)
            if not silence: 
                print('mean error:', np.nanmean(inputErr))
                print('stdDev of error:', stdDevErr)

        outliers = []

        if inputErr is not None:
            # Exclude datapoints with large errors
            badIndices = np.where( inputErr > stdDevErr*3.0 )[0]
            for index in badIndices:
                outliers.append(index)

            goodIndices = np.where( inputErr <= stdDevErr*3.0 )[0] 
            newMean = np.nanmean(inputData[goodIndices])
            newStdDev = np.nanstd(inputData[goodIndices])
            newStdDevErr = np.nanstd(inputErr[goodIndices])

            if not silence:
                print('new mean flux:', newMean)
                print('new stdDev flux:', newStdDev)
                print('new mean error:', np.nanmean(inputErr[goodIndices]))
                print('new stdDev of error:', newStdDevErr)

            hiRate = numStdDevs*newStdDev + newMean
            lowRate = newMean - numStdDevs*newStdDev

            # Find values outside a 3 standard deviation range of the input data 
            for i in range(0,len(inputData)-1,1):
                if not (i in outliers):
                    if inputData[i] > hiRate or inputData[i] < lowRate:
                        outliers.append(i)
        else:
            hiRate = numStdDevs*stdDev + mean
            lowRate = mean - numStdDevs*stdDev

            # Find values outside a 3 standard deviation range of the input data 
            for i in range(0,len(inputData)-1,1):
                if inputData[i] > hiRate or inputData[i] < lowRate:
                    outliers.append(i)
                    
        ts_no_outliers = []
        for i in range(0, len(inputData)):
            if i in outliers:
                ts_no_outliers.append(np.nan)
            else: 
                ts_no_outliers.append(inputData[i])
        ts_no_outliers = np.array(ts_no_outliers)

        # Now exclude NaN entries entirely
        rate = ts_no_outliers[~np.isnan(ts_no_outliers)]
        time = inputTimes[~np.isnan(ts_no_outliers)]
        bin_times = inputBinTimes[~np.isnan(ts_no_outliers)]
        error = inputErr[~np.isnan(ts_no_outliers)]

        self.time_series = rate
        self.times = time
        self.bin_times = bin_times
        self.errors = error
        
        return outliers
    
    
    def rebinLC(self, dt, multidim=False, multidimComp=0,
            tstart=None, tstop=None, minbin=1,
            user_bin=False, user_bin_lo=None, user_bin_hi=None,
            weight=False, delgap=0, interpolation_width=50, frac_time_threshold=0.2):
        '''
        Rebin a rate lightcurve
           Inputs:
             dt         : Chosen width of evenly spaced time bins in rebinned lightcurve

           Optional Input:
             multidim              : (bool) to indicate whether the time series is multi-dimensional (Default: False)
             multidimComp          : if the time series is multi-dimensional, specify which vector component to use (int)
             binTimes              : Total time observed in each bin, same units as dt
             tstart                : Beginning of first output time bin
             tstop                 : End of last output time bin
             minbin                : Minimum required events per bin, else the bin is set to 0,  or ignored. Default=1.
             user_bin              : Use custom time bins (True); Default: False
             user_bin_lo           : Lower time bounds for user defined grid
             user_bin_hi           : Upper time bounds for a user defined grid. Overrides  dt, tstart, and tstop inputs
             delgap                : If set, delete empty bins from the lightcurve, otherwise,  set them to 0
             weight                : If set and err input, the mean is weighted by the error.
                                     I.e., mean = (\sum rate/err^2)/(\sum 1/err^2), and the output variance becomes
                                     (\sum (mean-rate/err^2)^2)/(\sum 1/err^2)^2]}
             interpolation_width   : number of bins (which is in units of the input timing array) 
                                     e.g. if the input t array is in days, and bin size (dt) is 2, 
                                     then iw=2 means 2x2days = 4 days interpolation width
             frac_time_threshold   : required fractional observation time per bin; binTimes must be provided

           Outputs:
             rate       : Mean rate of resulting rebinning
             time       : Lower time bounds of rebinned lightcurve


           Optional Outputs:
             error     : Rate errors combined in quadrature, i.e., it's
                          sqrt{\sum{ err^2 }}/N, where N is the number of points
                          in that particular bin (i.e., lc.num).  Only computed if
                          err is input [otherwise, just use sqrt(lc.var)].
             bintimes  : The new total time observed in each bin
        '''
        t = self.times
        
        if multidim:
            rate = self.time_series.T[multidimComp]
        else:
            rate = self.time_series
        err = self.errors
        
        binTimes = self.bin_times
        
        ######################################
        # Settings
        ######################################
        if ( delgap !=0 and minbin == 0 ):
            minbin = 1

        custf = 0
        if ( user_bin ):
            custf = 1
            dt = 0

        if ( dt is None ):
            dt = 0

        ######################################
        # Ensure proper lengths of arrays
        ######################################
        if ( len(t) != len(rate) ):
            print("Incommensurate array lengths for time and rate. Exiting.")
            return

        mint = min(t)
        maxt = max(t)

        if ( tstart is None ):
            tstart = mint
        if ( tstop is None ):
            tstop = maxt
            
        # Start time array at 0   
        t = t - tstart

        if( maxt < tstart or mint > tstop or mint > maxt):
            print("Data outside of bounds of start & stop times:")
            print("\t[tstart,tstop] = [%e,%e]\n" %(tstart,tstop))
            print("\t[min_t, max_t] = [%e,%e]\n" %(mint,maxt))
            return

        if ( (err is not None) and (len(err) != len(rate)) ):
            print("Incommensurate array length for error and rate. Exiting.")
            return

        ######################################
        # Determine binning parameters for evenly sampled arrays
        ######################################
        if ( (custf == 0) and (dt > 0) ):
            nfrac = ( tstop - tstart )/dt
            nbins = int(nfrac)
            lc_bin_lo = np.arange(0, nbins-1, 1)*dt #+ tstart
            lc_bin_hi = np.arange(1, nbins, 1)*dt #+ tstart
        elif ( custf == 1 ):
            lc_bin_lo = user_bin_lo
            lc_bin_hi = user_bin_hi
        else:
            print("New time bin or custom bin array must be input.")
            return

        ######################################
        # Construct evenly spaced timing arrays of desired binning size
        ######################################
        lc_num, lc_bins = np.histogram(t, nbins) # number of t values (lc_num) in each bin (lc_bins)
        lc_bin_inds = np.digitize(t, lc_bins) # for each entry of t, find index of associated bin

        #lc_binned = np.zeros_like(lc_num) # empty array filled with binned values (interpolate empty entries)

        lc_non_zero_bins = np.where( lc_num > 0.0 )[0] # indices of lc_num where there are non-empty bins
        lc_non_zero_bin_inds = np.digitize(t, lc_non_zero_bins) # for each entry of t, find index of associated bin but only for non-empty ones

        ######################################
        # Fill in the new histograms/binned LC with avg values, fill gaps
        ######################################
        new_rates = []; new_errors = []; new_bintimes = []
        empty_bin_num = 0.0
        for i in range(1, len(lc_bins)-1, 1): # for each index of a bin
            inds_in_this_bin = np.where( lc_bin_inds == i )[0] # find t/rate indices in this bin
            obs_time_below_threshold = False # default

            # determine whether total time observed in this bin is below the threshold
            if binTimes is not None:
                fractional_observation_time = np.sum(binTimes[inds_in_this_bin])/dt
                if fractional_observation_time < frac_time_threshold:
                    obs_time_below_threshold = True
                else:
                    obs_time_below_threshold = False

            ###############
            # IF NON-EMPTY, MULTI-VALUE BIN 
            #     Compute new rate from mean of values in this bin
            if ( len(inds_in_this_bin) > 1 ) and ( obs_time_below_threshold == False ): #lc_num[i]
                if( (err is not None) and (weight == 0) ):
                    # average the rates in this bin without weighting
                    new_rate = np.mean(rate[inds_in_this_bin])
                    err_sqr = np.square(err[inds_in_this_bin])
                    new_err = np.sqrt(np.sum(err_sqr))/len(inds_in_this_bin)    #lc_num[i]

                if( (err is not None) and (weight != 0) ):
                    # compute the weighted mean
                    err_sqr = np.square(err[inds_in_this_bin])
                    err_sqr_sum = np.sum(1.0/err_sqr[err_sqr != 0.0])
                    new_rate = np.sum(rate[inds_in_this_bin]/err_sqr)/err_sqr_sum
                    new_err = 1.0/err_sqr_sum

                if( err is None ):
                    # compute mean; error derived from standard deviation about this mean
                    new_rate = np.nanmean(rate[inds_in_this_bin])
                    new_err = np.nanstd(rate[inds_in_this_bin])

                if( binTimes is not None ):
                    new_bintime = np.sum(binTimes[inds_in_this_bin])

                if( binTimes is None ):
                    new_bintime = dt

            ##############        
            # IF 1 VALUE IN BIN
            #     with only one value in this bin, the rate/error is what it is!
            elif ( len(inds_in_this_bin) == 1 ) and ( obs_time_below_threshold == False ):
                new_rate = rate[inds_in_this_bin]
                if( err is not None):
                    new_err = err[inds_in_this_bin]
                if( err is None ):
                    new_err = np.nanstd(rate[inds_in_this_bin])
                if( binTimes is not None ):
                    new_bintime = np.sum(binTimes[inds_in_this_bin])
                if( binTimes is None ):
                    new_bintime = dt

            ###############    
            # IF EMPTY BIN or BELOW BINTIME THRESHOLD: Interpolate
            #     if this bin is empty, insert random number sampled from surrounding mean+stddev
            elif ( len(inds_in_this_bin) < 1 ) or ( obs_time_below_threshold ) :
                empty_bin_num += 1.0 # Count the number of bins that are interpolated

                iw = interpolation_width # number of bins (which is in units of the input timing array); 
                # e.g. if the input t array is in days, and bin size is 2 days, then iw=2 means 2x2days interpolation width, or a width of 4 days
                bins_inds_left = np.searchsorted(lc_non_zero_bins, i) 
                bins_inds_right = np.searchsorted(lc_non_zero_bins, i, side='right')

                # If the value is in the middle of the light curve (not near edges), then sample
                #  the new value from the previous iw non-empty bins and next iw non-empty bins
                if (i>=iw) and (i < (len(lc_bins)-iw)):
                    bins_inds = lc_non_zero_bins[bins_inds_left-iw:bins_inds_right+iw]

                # If the value is near the beginning of the light curve, sample from the next 2*iw non-empty bins
                if i<iw: 
                    bins_inds = lc_non_zero_bins[bins_inds_left:bins_inds_right+iw]

                # If the value is near the end of the light curve, sample from the last 2*iw non-empty bins
                if i > (len(lc_bins)-iw): 
                    bins_inds = lc_non_zero_bins[bins_inds_left-iw:bins_inds_right]

                # find index of t/rate associated with each surrounding non-zero bin
                inds_in_surrounding_bins = [ np.where( lc_bin_inds == ii )[0] for ii in bins_inds ] 
                #inds_in_surrounding_bins = list(flatten(inds_in_surrounding_bins))
                inds_in_surrounding_bins = list(chain.from_iterable(inds_in_surrounding_bins))
                #inds_in_surrounding_bins = list(np.concatenate(inds_in_surrounding_bins).ravel())

                # Compute mean and standard deviation from surrounding non-zero bins
                surrounding_mean = np.nanmean(rate[inds_in_surrounding_bins])
                surrounding_err = np.nanstd(rate[inds_in_surrounding_bins])

                new_rate = np.random.normal(surrounding_mean, surrounding_err, 1)[0]
                new_err = surrounding_err

                if( binTimes is not None ):
                    new_bintime = np.sum(binTimes[inds_in_surrounding_bins])
                if( binTimes is None ):
                    new_bintime = dt

            new_rates.append( new_rate )
            new_errors.append( new_err )
            new_bintimes.append( new_bintime )

        new_rates=np.array(new_rates); new_errors=np.array(new_errors); new_bintimes=np.array(new_bintimes)
        # Make timing array which is centered in each bin
        new_t = (lc_bin_lo + lc_bin_hi)/2.0 + tstart
        new_t = np.array(new_t)

        # Flatten these messy rates & errors lists/arrays
        #     temp_rates = []
        #     for sub_item in new_rates:
        #         temp_rates.append( list(sub_item.ravel()) )
        #     temp_errs = []
        #     for sub_item in new_errors:
        #         temp_errs.append( list(sub_item.ravel()) )

        #     new_rates = [y for x in temp_rates for y in x]
        #     new_errors = [y for x in temp_errs for y in x]
        

        self.times = new_t
        self.time_series = new_rates
        self.errors = new_errors

        return new_bintimes, np.array(empty_bin_num)


    def subSampleLC(self, sample_rate=1):
        """
        sampleLC -- This extracts the flux at every user-defined number of time steps.
        Input: 
            sampling_rate :  (int) the number of timesteps, in indices, between each successive observation (default: 1)
        Output:
            new_t : (array) the new times array
            new_x : (array) the new time series observations
        """
        t = self.times
        x = self.time_series
        e = self.errors
        
        new_t = t[::sample_rate]
        new_x = x[::sample_rate]
        if e is not None:
            new_e = e[::sample_rate]
            return new_t, new_x, new_e
        else:
            return new_t, new_x


    def getFFT(self, timeStep, multidim=False, multidimComp=0, smoothSignal=False, boxSmoothNum=1, normScheme='rms-squared', surrogate=False, surrogate_ts=None):
        """ getFFT -- This computes the fft of the input data and returns the power spectrum (fft, power, frequencies, and periods)
            Input: 
                multidim     : (bool) to indicate whether the time series is multi-dimensional (Default: False)
                multidimComp : if the time series is multi-dimensional, specify which vector component to use (int)
                smoothSignal : (bool) convolve the time series with a Box1DKernel, the width specified by boxSmoothNum (default: False)
                boxSmoothNum : (int) if smoothSignal=True, then set the width of the Box1DKernel with this parameter (default: 1)
                normScheme   : (str) how to normalize the power in each window; 
                                'None': don't normalize; 
                                'max': divide by maximum power of entire time series;
                                'rms-squared': (default) fourier squared power is multiplied by 2*T/(mu^2 N^2) where T is the duration of the light curve,
                                mu is the mean flux, and N is the total number of data points. The mean should be subtracted first to remove zero-frequency power
                surrogate    : (bool) set to True if providing a surrogate time series rather than the provided Time_Series instance (default: False)
                surrogate_ts : (array) if surrogate=True, provide a numpy array of the surrogate time series (default: None)
            Output: 
                signal_fft : the FFT of the time series
                power_norm : the absolute value of the FFT squared
                freqs      : the positive axis of frequencies
                period     : corresponding periods (1/freqs) to the output frequencies
        """
        if surrogate:
            ts = surrogate_ts
        else:
            ts = self.time_series
                        
        if multidim:
            ts = ts.T[multidimComp]
        
        # Smooth the input data first to reduce large deviations
        if smoothSignal:
            signal =  convolve(np.array(ts), Box1DKernel(boxSmoothNum))
        else:
            signal = np.array(ts)

        # Create an array of the frequency values from the time step and size of
        # the input data array
        sample_freq = fftpack.fftfreq(signal.size, d=timeStep)

        # Calculate the FFT of the input data
        signal_fft = fftpack.fft(signal)

        # Extract the positive axis of frequencies only
        posXs = np.where(sample_freq > 0)
        freqs = sample_freq[posXs]

        # Calculate the power of the FFT signal (absolute value of the FFT squared)
        # TO DO: change the normalization
        power = (np.abs(signal_fft[posXs]))**2
        if normScheme == 'max':
            power_norm = np.array([x/max(power) for x in power])
        elif normScheme == 'rms-squared':
            mu = np.mean(signal)
            N = len(signal)
            T = N*timeStep
            power_norm = power * 2.0 * T / (mu*mu * N*N)
        else:
            power_norm = power

        # Determine the period corresponding to each frequency
        period = 1.0/freqs

        return signal_fft, power_norm, freqs, period


    
    def movingFFT(self, timeStep, segmentLength, multidim=False, multidimComp=0, smoothSignal=False, boxSmoothNum=1, normScheme='rms-squared', surrogate=False, surrogate_ts=None):
        """ movingFFT -- computes the power spectrum in a moving window across the time series
            Input: 
                segmentLength : (int) size of the window, expected in indices
                multidim     : (bool) to indicate whether the time series is multi-dimensional (Default: False)
                multidimComp : if the time series is multi-dimensional, specify which vector component to use (int)
                smoothSignal : (bool) convolve the time series with a Box1DKernel, the width specified by boxSmoothNum (default: False)
                boxSmoothNum : (int) if smoothSignal=True, then set the width of the Box1DKernel with this parameter (default: 1)
                normScheme   : (str) how to normalize the power in each window; 
                                    'None': don't normalize; 
                                    'max': divide by maximum power of entire time series;
                                    'rms-squared': (default) fourier squared power is multiplied by 2*T/(mu^2 N^2) where T is the duration of the light curve,
                                    mu is the mean flux, and N is the total number of data points. The mean should be subtracted first to remove zero-frequency power
                surrogate    : (bool) set to True if providing a surrogate time series rather than the provided Time_Series instance (default: False)
                surrogate_ts : (array) if surrogate=True, provide a numpy array of the surrogate time series (default: None)
            Output: 
                power      : the absolute value of the FFT squared, normalized by the maximum power
                freqs      : the positive axis of frequencies
                period     : corresponding periods (1/freqs) to the output frequencies
        """      
        if surrogate:
            ts = surrogate_ts
        else:
            ts = self.time_series
            
        if multidim:
            ts = ts.T[multidimComp]
        
        # Smooth the input data first with boxcar filter
        if smoothSignal:
            signal =  convolve(np.array(ts), Box1DKernel(boxSmoothNum))
        else:
            signal = np.array(ts)
            
        # Subtract the mean
        signal = signal - np.mean(signal)
            
        newSig = []
        
        # Pad the ends of the data with noise
        stdDev = ts.std
        print('stdDev = ', stdDev)
        
        for i in range(0,segmentLength,1):
            randNum = 0.01*randrange(-int(stdDev*100),int(stdDev*100),1)
            newSig.append(randNum)
        for i in range(0,len(signal),1):
            newSig.append(signal[i])
        for i in range(0,segmentLength,1):
            randNum = 0.01*randrange(-int(stdDev*100),int(stdDev*100),1)
            newSig.append(randNum)

        power = []; freqs = []; period = []
        for i in range(0,(len(newSig) - segmentLength),1):
            FFT_Data = Time_Series.getFFT(newSig[i:i+segmentLength], timeStep)
            power.append(FFT_Data[1]); freqs.append(FFT_Data[2]); period.append(FFT_Data[3])

        return power, freqs, period
    
    
    def derivative(self):
        '''
        Returns numerical derivative via average of backward and forward finite difference
        (version of central differencing)
        The ends are treated with either a forward or backward finite difference
        
        Does not handle multi-dimensional time series
        
        Inputs: None
        Outputs: derivative (numpy array), center times of each derivative (numpy array)
        '''
        x = self.time_series
        t = self.times
        
        x = np.atleast_1d(x)
        if len(x.shape) != 1:
            raise ValueError("invalid dimensions for numerical differentiation")
        
        x_prime = []; t_prime = []
        for n in range(0,len(x)-1,1):
            if n == 0:
                deriv = (x[n+1] - x[n]) / (t[n+1] - t[n])
                x_prime.append(deriv)
                t_new = (t[n+1] + t[n])/2
                t_prime.append(t_new)
            elif n == len(x)-1:
                deriv = (x[n] - x[n-1]) / (t[n] - t[n-1])
                x_prime.append(deriv)
                t_new = (t[n] + t[n-1])/2
                t_prime.append(t_new)
            else:
                x_prime_lower = (x[n] - x[n-1]) / (t[n] - t[n-1])
                x_prime_upper = (x[n+1] - x[n]) / (t[n+1] - t[n])
                deriv = (x_prime_lower + x_prime_upper) / 2.0
                t_new = t[n]
                x_prime.append(deriv)
                t_prime.append(t_new)

        return [x_prime, t_prime]
    
    
    @staticmethod
    def next_pow_two(n):
        i = 1
        while i < n:
            i = i << 1
        return i

    
    def autocorrelation_function(self, norm=True):
        """
        Computes the 1D autocorrelaion function of a scalar time series derived from the inverse fourier transform
        Inputs:
            norm : (bool) optionally normalize by the first delay of the ACF
        """
        x = self.time_series
        x = np.atleast_1d(x)
        if len(x.shape) != 1:
            raise ValueError("invalid dimensions for 1D autocorrelation function")
        n = Time_Series.next_pow_two(len(x))

        # Compute the FFT and then (from that) the auto-correlation function
        f = np.fft.fft(x - np.mean(x), n=2*n)
        acf = np.fft.ifft(f * np.conjugate(f))[:len(x)].real
        acf /= 4*n

        # Optionally normalize
        if norm:
            acf /= acf[0]

        return acf
                        
                        
    def mutual_information(self, maxdelay=100, bins=64, multidim=False, multidimComp=0):
        """
        Calculate the mutual information between time series and its delayed copy as a function of delay.
        Mutual information defined as I = S(x) + S(y) - S(x,y), between x[:-delay] and y=x[delay:], 
        where S(x) is the Shannon entropy.
        
        Scalar time series only.
        
        Inputs:
            bins         : (int) number of bins to use while creating the histogram.
            maxdelay     : (int) compute mutual information out to a maximum delay, default=100
            multidim     : (bool) set whether the time series is multi-dimensional (Default: False)
            multidimComp : if the time series is multi-dimensional, specify which vector component to use (int)
        
        Outputs:
            delay        : (int) delay used to calculate the mutual informaiton
            multual_info : (float) mutual information
        """
        
        ts = np.atleast_1d(self.time_series)
        if len(ts.shape) != 1:
            raise ValueError("invalid dimensions for 1D autocorrelation function")
        n = Time_Series.next_pow_two(len(ts))
        
        mutual_info = []; delays = []
        for i in range(1,maxdelay):
            x = ts[:-i]
            y = ts[i:]
                        
            p_x = np.histogram(x, bins)[0]
            p_y = np.histogram(y, bins)[0]
            p_xy = np.histogram2d(x, y, bins)[0].flatten()

            # Convert frequencies into probabilitiesm 
            p_x = p_x[p_x > 0] / np.sum(p_x)
            p_y = p_y[p_y > 0] / np.sum(p_y)
            p_xy = p_xy[p_xy > 0] / np.sum(p_xy)

            # Calculate the corresponding Shannon entropies.
            h_x = np.sum(p_x * np.log2(p_x))
            h_y = np.sum(p_y * np.log2(p_y))
            h_xy = np.sum(p_xy * np.log2(p_xy))
                        
            mutual_info.append(h_xy - h_x - h_y)
            delays.append(i)

        return delays, mutual_info
                        
                        
                        
    def adfd(self, dim=1, maxdelay=100, metric='euclidean'):
        """
        Compute average displacement from the diagonal (ADFD).
        Computes the average displacement of the time-delayed vectors from
        the phase space diagonal (Rosenstein et al. 1994).
        
        Applies to scalar time series only.
        
        Inputs:
            dim      : (int) embedding dimension (default = 1)
            maxdelay : (int) the maximum time delay from diagonal to consider
            metric   : (str) metric for determining phase space distances
            
        Outputs:
            displacement : (array) ADFD as a function of time delays (up to maxdelay)
        """                
        ts = self.time_series
        ts = np.atleast_1d(ts)
        if len(ts.shape) != 1:
            raise ValueError("invalid dimensions for ADFD (requires scalar time series)")
                        
        displacement = np.zeros(maxdelay)
        N = len(ts)

        # use max delay or size of time series (whichever is smaller)
        maxdelay = min(maxdelay, int(N / dim))

        for delay in range(1, maxdelay):
            # construct time delay embedding of time series
            x = Time_Series.embed(ts, dimension=dim, delay=delay)

            # Reconstruct with zero time delay.
            y = ts[:N - (dim - 1) * delay]
            y = y.repeat(dim).reshape(len(y), dim)

            func = getattr(distance, metric) # from scipy.spatial.distance
            distances = np.asarray([func(i, j) for i, j in zip(x, y)])
            displacement[delay] = np.mean(distances)

        return displacement
                        
    @staticmethod   
    def nearest_neighbors(ts, metric='euclidean', theiler_window=0, max_neighbors=None):
        """
        Find the nearest neighbors in phase space of a given embedded time series.
        The nearest neighbors are determined using SciPy's KDTree search.
        
        Inputs:
            ts             :  (ndarray) Embedded time series (multi-dimensional from time delay method)
            metric         :  (str) The metric used to determine distances in phase space.
                                    Options include 'cityblock', 'euclidean' or 'chebyshev'
            theiler_window :  (int) This is the Theiler window, a minimum separation in time between points
                                    to ignore points in the time series which are correlated
            max_neighbors  :  (int) The maximum number of near neighbors that should be found for each
                                    point in phase space
        Outputs:
            indices, distances : for all nearest neighbors found up to max_neighbors
        """ 
        # For using the KDTree, we must determine which Minkowski p-norm to use: 
        # 1 is the sum-of-absolute-values “Manhattan” distance 
        # 2 is the usual Euclidean distance 
        # infinity is the maximum-coordinate-difference distance 
        # A large, finite p may cause a ValueError if overflow can occur.
        if metric == 'cityblock':
            p = 1
        elif metric == 'euclidean':
            p = 2
        elif metric == 'chebyshev':
            p = np.inf
        else:
            raise ValueError("Unknown metric. Choose one of 'cityblock', 'euclidean, or 'chebyshev'.")

        # Set up the KDtree: for quick nearest-neighbor lookup
        #  provides an index into a set of k-dimensional points which can be used to rapidly look up the nearest neighbors of any point.
        tree = KDTree(ts)
        N = len(ts)

        # Enforce maximum number of neighbors to be larger than the theiler window (if not set)
        # Throw error if number of neighbors is larger than the actual input time series
        if not max_neighbors:
            max_neighbors = (theiler_window + 1) + 1 + (theiler_window + 1)
        else:
            max_neighbors = max(1, max_neighbors)

        if max_neighbors >= N:
            raise ValueError("Maximum number of near neighbors that should be found for each point in phase space (max_neighbors) is bigger than array length.")

        # Set up placeholders for the distances and indices of the neighbors
        distances = np.empty(N)
        indices = np.empty(N, dtype=int)

        for i, x in enumerate(ts):
            for k in range(2, max_neighbors + 2):
                                                
                # Query the k nearest neighbors in phase space, and get their index & distance from
                # this phase space position (i, x)
                dist, index = tree.query(x, k=k, p=p)
                        
                # determine whether the index of the neighbor falls within the theiler window
                # if the distance is non-zero and time position greater than the theiler window, 
                # then this is a 'valid' neighbor
                valid = (np.abs(index - i) > theiler_window) & (dist > 0)

                # if the number of 'valid' neighbors is non-zero, then save their distance and index
                # information and move on
                if np.count_nonzero(valid):
                    distances[i] = dist[valid][0]
                    indices[i] = index[valid][0]
                    break

                # if there are no valid neighbors once we have reached the maximum number of neighbors to 
                # search for, then throw an error
                if k == (max_neighbors + 1):
                    raise Exception('Could not find any near neighbor with a nonzero distance. Try increasing the value of max_neighbors.')

        return np.squeeze(indices), np.squeeze(distances)
                        
                        
    def false_nearest_neighbors(self, delay=1, maxdim=10, tolerance_I=10.0, tolerance_II=2.0, metric='euclidean', theiler_window=1, max_neighbors=None):
        """
        Compute the fraction of false nearest neighbors using methods from Kennel et al. (1992)
        This algorithm is used to determine the optimal dimension for a time series embedded using
        the time delay method.
        
        Inputs:
            delay          :  (int) Embedding time delay for the time series
            maxdim         :  (int) The maximum dimension to compute false neighbors
            tolerance_I    :  (float) The threshold to determine a false neighbor for test I
            tolerance_II   :  (float) The threshold to determine a false neighbor for test II
            metric         :  (str) The metric used to determine distances in phase space.
                                    Options include 'cityblock', 'euclidean' or 'chebyshev'
            theiler_window :  (int) This is the Theiler window, a minimum separation in time between points
                                    to ignore points in the time series which are correlated
            max_neighbors  :  (int) The maximum number of near neighbors that should be found for each
                                    point in phase space
        Outpus:
            Returns numpy arrays that are fraction of false nearest neighbors by Test I, Test II, and the max(Test I, Test II)
        
        The ratio of distances between nearest neighbors with an increase in embedding dimension
        is defined as: 
          a(i,d) = || y_i(d+1)-y_j(d+1) || / || y_i(d) - y_j(d) ||
        where y_i and y_j are nearest neighbors (using helper function Time_Series.nearest_neighbors) in embedding dimension d, 
        and ||.|| is the metric distance.
        
        For test I:
        The threshold, a(i,d), dictates whether these neighbors are 'false' -- set by the input parameter tolerance_I
        If the neighbors are greater than this threshold, they are considered 'false'.
        
        For test II:
        The relative distance between each pair of points identified as neighbors is compared between dimension, d, and d+1. 
        If the ratio of the relative distances to the standard deviation of the input time series is greater than the tolerance_II parameter,
        then these neighbors are considered 'false'.
        
        Inputs:
            maxdim (int) : the maximum dimension to compute false nearest neighbors
            tolerance (float) : the tolerance parameter to determine a false neighbor (TEST I)
                        i.e. distances(dim+1) / distances(dim) > tolerance
            A (float) : the tolerance paramter to determine a false neighbor (TEST 2)
                        i.e. (distances(dim+1)) / (standard deviation of the time series (dim)) > tolerance
        """
        ts = self.time_series
        dims = np.arange(1, maxdim+1)       
        fnn_1s=[]; fnn_2s=[]; max_fnns=[]
        for d in dims:
                        
            # Embed the time series for the CURRENT dimension and delay
            # (ending ts at -delay ensures it is the same length as the dim+1 case)
            y1 = Time_Series.embed(ts[:-delay], d, delay)
                        
            # Embed the time series for the NEXT dimension, and delay
            y2 = Time_Series.embed(ts, d + 1, delay)

            # Find nearest neighbors and their distances in the current dimension 
            index, this_distance = Time_Series.nearest_neighbors(y1, metric=metric, theiler_window=theiler_window,
                                          max_neighbors=max_neighbors)

            # Find all potential false neighbors using Kennel et al. (1992) tests
            # Test I: Find the distances to all neighbors for next dimension, 
            #  and divide by the distances to neighbors for the current dimension
            # False neighbors are those above the tolerance of this ratio
            fnn_1 = np.abs(y2[:, -1] - y2[index, -1]) / this_distance > tolerance_I
                        
            # Test II: Find the distances to all neighbors for the next dimension,
            #  and divide by the standard deviation of the time series in the current dimension
            # False neighbors are those above the tolerance of this ratio
            dist_func = getattr(distance, metric) # from scipy.spatial.distance   
            next_distances = np.asarray([dist_func(i, j) for i, j in zip(y2, y2[index])])
            fnn_2 = next_distances / np.std(ts) > tolerance_II
                        
            # The maximum of test 1 or 2
            max_fnn = fnn_1 | fnn_2

            fnn_1s.append(np.mean(fnn_1)); fnn_2s.append(np.mean(fnn_2)); max_fnns.append(np.mean(max_fnn))
                        
        return np.asarray(fnn_1s), np.asarray(fnn_2s), np.asarray(max_fnns)
                        
                        
    def avg_false_neighbors(self, delay=1, maxdim=10, metric='euclidean', theiler_window=0, max_neighbors=None):
        """
        Compute the averaged false neighbors method by Cao (1997) to estimate the minimum embedding dimension
        required for embedding the time series with the time delay method.
        
        Inputs:
            delay (int)          :  Embedding time delay for the time series
            maxdim (int)         :  The maximum dimension to compute false neighbors
            metric (str)         :  The metric used to determine distances in phase space.
                                    Options include 'cityblock', 'euclidean' or 'chebyshev'
            theiler_window (int) :  This is the Theiler window, a minimum separation in time between points
                                    to ignore points in the time series which are correlated
            max_neighbors (int)  :  The maximum number of near neighbors that should be found for each
                                    point in phase space
        Outpus:
            Returns E(dim), E^*(dim) for the AFN method as a function of dimension (dim).
            
        Similar to FNN algorithm, the ratio of distances between nearest neighbors with an increase in embedding dimension
        is defined as: 
          a(i,d) = || y_i(d+1)-y_j(d+1) || / || y_i(d) - y_j(d) ||
        where y_i and y_j are nearest neighbors in embedding dimension d, and ||.|| is the metric distance

        In standard FNN, there is a single threshold used for a(i,d) to determine whether points are false neighbors
        However - the threshold should ideally be determined by the derivative of the underlying system, and is therefore
        different for each phase point. We therefore consider the average false neighbor threshold, defined as E(d):
           E(d) = 1/(N-d*tau) SUM{  a(i,d)  }{ for i=1 to N-d*tau}
        which is dependent only on dimension d and delay tau.

        To investigate the variation from E(d) to E(d+1), compute E1(d) = E(d+1)/E(d)
         This quantity stops changing above some dimension d0, if the time series comes from an attractor.
         Thus, d0+1 is the mininum embedding we are looking for.

        To distinguish deterministic signals from stochastic signals, a second quantity is defined:
           E*(d) = 1/(N-d*tau) SUM{  || x_i - x_j ||  }{ for i=1 to N-d*tau}
         where x_i and x_j are nearest neighbors, ||.|| is the metric distance between neighbors, and we have thus
         defined the average distance between nearest neighbors.

        In a similar fashion, to investigate the variation of E*(d) to E*(d+1), compute E2(d) = E*(d+1)/E*(d)
        For a time series from a random set of numbers, E1(d) never saturates.
        For random data, since the future values are independent of the past values, E2(d) will be equal to 1 for any d in this case. 
        However, for deterministic data, E2(d) is certainly related to d, as a result, it cannot be a constant for all d; 
        in other words, there must exist some d's such that E2(d) is not equal to 1.
    
        """     
        ts = self.time_series
        
        dims = np.arange(1, maxdim+1)       
        afns_E=[]; afns_Es=[]
        for d in dims:
                        
            # Embed the time series for the CURRENT dimension and delay
            # (ending ts at -delay ensures it is the same length as the dim+1 case)
            y1 = Time_Series.embed(ts[:-delay], d, delay)
                        
            # Embed the time series for the NEXT dimension, and delay
            y2 = Time_Series.embed(ts, d + 1, delay)

            # Find nearest neighbors and their distances in the current dimension 
            index, this_distance = Time_Series.nearest_neighbors(y1, metric=metric, theiler_window=theiler_window,
                                          max_neighbors=max_neighbors)

            # Compute the magnification and the increase in the near-neighbor
            # distances and return the averages.
            dist_func = getattr(distance, metric) # from scipy.spatial.distance   
            next_distances = np.asarray([dist_func(i, j) for i, j in zip(y2, y2[index])])
            E = next_distances / this_distance
            Es = np.abs(y2[:, -1] - y2[index, -1])

            afns_E.append(np.mean(E)); afns_Es.append(np.mean(Es))
        
        return np.asarray(afns_E), np.asarray(afns_Es)

    @staticmethod
    def end_to_end_mismatch(ts, multidim=False, multidimComp=0):
        """
        Finds the segment in the time series that has minimum end-point mismatch.  
        Calculate the mismatch between the end points of all segments of the given length and pick the segment with
        least mismatch (Ehlers et al. 1998).  
        Enforce the condition that the difference between the first derivatives at the end points must be a minimum.

        Inputs:
            ts (array) : input time series
        Outputs:
            ends (tuple) : indices of the end points of the segment.
            d (float) : discontinuity statistic for the segment.
        Notes
        -----
        Both the time series and its first difference are linearly rescaled
        to [0, 1].  The discontinuity statistic varies between 0 and 1
        (0 means no discontinuity and 1 means maximum discontinuity).
        """
        
        weight = 0.5 #weight given to discontinuity in the 1st difference (b/t 0 and 1)
        neigh = 3 #num of end points used which the discontinuity statistic should be computed
                              
        # Calculate the first difference of the time series 
        diff = np.diff(ts)
        # Rescale the first difference and time series to [0,1]
        dx = (diff - np.min(diff)) / (np.max(diff) - np.min(diff))
        x = ((ts - np.min(ts)) / (np.max(ts) - np.min(ts)))[1:]
        n = len(x)

        # length of time series to be used 
        primes = np.array([2, 3, 5, 7, 11])
        i = np.argmax(primes ** np.floor(np.log(n) / np.log(primes)) - n)
        length = int(primes[i] ** (np.floor(np.log(n) / np.log(primes[i]))))

        d = np.zeros(n - (length + neigh))

        for i in np.arange(n - (length + neigh)):
            d[i] = ((1 - weight) * (np.mean((x[i:i + neigh] -
                                    x[i + length:i + length + neigh]) ** 2.0)) +
                    weight * (np.mean((dx[i:i + neigh] -
                              dx[i + length:i + length + neigh]) ** 2.0)))

        return (1 + np.argmin(d), 1 + np.argmin(d) + length), np.min(d)

                        
    def shuffled_surrogate(self, num_surrs=1, multidim=False, multidimComp=0):
        """
        Returns shuffled surrogates that preserve the probability distribution.
        The realizations of white noise are consistent with the input time series amplitude distribution.
        Inputs:
            num_surrs (int) : number of surrogates to generate
            multidim (bool): True (time series is multi-dimensional); Default: False
            multidimComp (int) : if the time series is multi-dimensional, specify which vector component to use
        Outputs:
            surrs (ndarray) : num_surrs numbers of random surrogates with the same distribution as x.
        """
        if multidim:
            ts = self.time_series.T[multdimComp]
        else:
            ts = self.time_series
        N = len(ts)
        surrs = np.zeros((num_surrs, N))
                        
        for i in range(num_surrs):
            # select a random index from the time series
            j = np.random.permutation(np.arange(N))
            surrs[i] = ts[j]

        return surrs

    def phase_surrogate(self, num_surrs=1, multidim=False, multidimComp=0):
        """
        Returns phase randomized surrogates that preserve the power spectrum of the original time series.
        The surrogates are generated by taking the Fourier transform, randomizing the phases,
        and then inverse Fourier transform to generate a time series with the same autocorrelations, but with 
        the probability (amplitude) distribution destroyed.
        
        Inputs:
            num_surrs (int) : number of surrogates to generate
            multidim (bool): True (time series is multi-dimensional); Default: False
            multidimComp (int) : if the time series is multi-dimensional, specify which vector component to use
        Outputs:
            surrs (ndarray) : num_surrs numbers of random surrogates with the same distribution as x.
        """
        if multidim:
            ts = self.time_series.T[multdimComp]
        else:
            ts = self.time_series
        N = len(ts)
        surrs = np.zeros((num_surrs, N))
                        
        for i in range(num_surrs):
            y = np.fft.rfft(ts)
            phi = 2 * np.pi * np.random.random(len(y))

            phi[0] = 0.0
            if N % 2 == 0:
                phi[-1] = 0.0

            y = y * np.exp(1j * phi)
            surrs[i] = np.fft.irfft(y, n=N)

        return surrs
                        
    
    def AAFT_surrogates(self, num_surrs=1, multidim=False, multidimComp=0):
        """
        Return surrogates using the amplitude adjusted Fourier transform method (Schreiber 2000).
        Generates a phase randomized surrogate rescaled to the amplitude distribution of the data, preserving
        both the distribution and the power spectrum.
        Inputs:
            num_surrs (int) : number of surrogates to generate
            multidim (bool): True (time series is multi-dimensional); Default: False
            multidimComp (int) : if the time series is multi-dimensional, specify which vector component to use
        Outputs:
            surrs (ndarray) : num_surrs numbers of random surrogates with the same distribution as x.
        """
        if multidim:
            ts = self.time_series.T[multdimComp]
        else:
            ts = self.time_series
        N = len(ts)

        surrs = np.zeros((num_surrs, N))
                        
        for i in range(num_surrs):
                        
            #  Create sorted Gaussian reference series
            gaussian = random.randn(ts.shape[0], ts.shape[1])
            gaussian.sort(axis=1)

            #  Rescale data to Gaussian distribution
            ranks = ts.argsort(axis=1).argsort(axis=1)
            rescaled_data = np.zeros(ts.shape)

            for j in range(ts.shape[0]):
                rescaled_data[j, :] = gaussian[j, ranks[j, :]]

            #  Phase randomize rescaled data
            y = np.fft.rfft(rescaled_data)
            phi = 2 * np.pi * np.random.random(len(y))
            phi[0] = 0.0
            if len(x) % 2 == 0:
                phi[-1] = 0.0
            y = y * np.exp(1j * phi)
            phase_data = np.fft.irfft(y, n=N)

            #  Rescale back to amplitude distribution of original data
            sorted_original = ts.copy()
            sorted_original.sort(axis=1)

            ranks = phase_data.argsort(axis=1).argsort(axis=1)

            for j in range(ts.shape[0]):
                rescaled_data[j, :] = sorted_original[j, ranks[j, :]]
            
            surrs[i] = rescaled_data

        return surrs


    def iaaft_surrogate(self, num_surrs=1, multidim=False, multidimComp=0, maxiter=1000, atol=1e-8, rtol=1e-10):
        """
        Return surrogates using the iterative amplitude adjusted Fourier transform method (Schreiber & Schmitz 1996).
        Generates a phase randomized surrogate rescaled to the amplitude distribution of the data, preserving
        both the distribution and the power spectrum.
        Inputs:
            num_surrs (int) : number of surrogates to generate
            multidim (bool): True (time series is multi-dimensional); Default: False
            multidimComp (int) : if the time series is multi-dimensional, specify which vector component to use
            maxiter (int) : Maximum iterations to be performed while checking for convergence (Default = 1000)
            atol (float) : Absolute tolerance for checking convergence (default = 1e-8)
            rtol (float) : Relative tolerance for checking convergence (default = 1e-10)  
        Outputs:
            surrs (ndarray) : num_surrs number of surrogates 
            i (int) : number of iterations performed
            e (float) : root-mean-square deviation between the absolute squares of the Fourier amplitudes of the 
            surrogates and that of the original time series

        To check if the power spectrum has converged, we see if the absolute difference between the current (cerr) 
        and previous (perr) RMSDs is within the limits set by the tolerance levels, 
            i.e., if abs(cerr - perr) <= atol + rtol*perr.  
        Additionally, atol and rtol can be both set to zero in which case the iterations end only when the 
        RMSD stops changing or when maxiter is reached.
        """
        if multidim:
            ts = self.time_series.T[multdimComp]
        else:
            ts = self.time_series
        N = len(ts)

        surrs = np.zeros((num_surrs, N))
        num_iterations = np.zeros((num_surrs, 1))
        rms_deviations = np.zeros((num_surrs, 1))
                        
        for i in range(num_surrs):
                        
            # Calculate "true" Fourier amplitudes and sort the series.
            ampl = np.abs(np.fft.rfft(ts))
            sort = np.sort(ts)

            # Previous and current error.
            perr, cerr = (-1, 1)

            # Start with a random permutation.
            t = np.fft.rfft(np.random.permutation(ts))

            for j in range(maxiter):
                # Match power spectrum.
                s = np.real(np.fft.irfft(ampl * t / np.abs(t), n=N))

                # Match distribution by rank ordering.
                y = sort[np.argsort(np.argsort(s))]

                t = np.fft.rfft(y)
                cerr = np.sqrt(np.mean((ampl ** 2 - np.abs(t) ** 2) ** 2))

                # Check convergence.
                if abs(cerr - perr) <= atol + rtol * abs(perr):
                    break
                else:
                    perr = cerr

            # Normalize error w.r.t. mean of the "true" power spectrum.
            surrs[i] = y
            num_iterations[i] = j
            rms_deviations[i] = cerr / np.mean(ampl ** 2)
                        
        return surrs, num_iterations, rms_deviations

    
# Some TO-DOs

###########
# Functions to add artificial noise to time series
###########

###########
# Legendre polynomials for embedding of time series : alternative to time delay embedding
###########

###########
# Non-euclidean metric recurrence plot for irregularly spaced time series
###########

###########
# Lomb-scargle periodogram
###########

###########
# Nonlinear tests with the surrogate data method:
#    1) time reversal asymmetry
#    2) simple nonlinear prediction
###########
